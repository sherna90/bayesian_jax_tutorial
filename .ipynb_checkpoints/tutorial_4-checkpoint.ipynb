{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For Gaussians with $N(0,1)$ priors, we can calculate this as (see Kingma Welling 2013 VAE paper in the appendix)\n",
    "$$\n",
    "    KL(q(\\mu_1|\\sigma_1) || N(0,1)) = -\\frac{1}{2}  (1 + log(\\sigma^2) - \\mu^2 - \\sigma^2) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9013877\n",
      "4.90138771133189\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import distrax\n",
    "\n",
    "mu = 2.\n",
    "s  = 3.\n",
    "\n",
    "distrax.Normal(loc=2.0,scale=3.0).kl_divergence(distrax.Normal(loc=0.0,scale=1.0))\n",
    "print(-(1. + np.log(s**2) - mu**2 - s**2)/2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distrax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "key1,key2=jax.random.split(key)\n",
    "X = jnp.concatenate([jnp.ones(100).reshape(-1,1),jax.random.normal(key1,shape=(100,1))],axis=-1)\n",
    "beta=jnp.array([2,1]).reshape(-1,1)\n",
    "y = X@beta + 0.1 * jax.random.normal(key,shape=(100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array(-2043.5426, dtype=float32), Array([ 754422081, 3987528881], dtype=uint32))\n"
     ]
    }
   ],
   "source": [
    "sigma=0.1\n",
    "\n",
    "def log_likelihood(test_point):\n",
    "    #log_prior_eta=distrax.Normal(0.,1.).log_prob(test_point['eta']).sum()\n",
    "    #log_prior_mu=distrax.Normal(0.,1.).log_prob(test_point['mu']).sum()\n",
    "    #log_prior_tau=distrax.Transformed(distrax.Normal(loc=0., scale=1.),\n",
    "    #                                distrax.Lambda(lambda x:jnp.exp(x))).log_prob(test_point['tau']).sum()\n",
    "    loc=(test_point['mu']+test_point['tau']*test_point['eta']).reshape(-1,1)\n",
    "    log_like=distrax.Independent(distrax.Normal(loc=X@loc,scale=sigma)).log_prob(y).sum()\n",
    "    return log_like\n",
    "\n",
    "\n",
    "def elbo(test_point,key):\n",
    "    key1,key2=jax.random.split(key)\n",
    "    q_dist=distrax.Normal(test_point['mu'],test_point['tau'])\n",
    "    test_point.update({'eta':distrax.Normal(0.,1.).sample(seed=key1,sample_shape=(2,))})\n",
    "    loss=log_likelihood(test_point)+q_dist.kl_divergence(distrax.Normal(0.,1.)).sum()\n",
    "    return loss,key2\n",
    "\n",
    "\n",
    "\n",
    "test_point={'mu':jnp.array([0.0,0.0]),'tau':jnp.array([1.0,1.0]),'eta':jnp.array([1.0,1.0])}\n",
    "print(elbo(test_point,key2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayeux as bx \n",
    "\n",
    "def transform_fn(test_point):\n",
    "  return {'mu': test_point['mu'], 'tau': jnp.exp(test_point['tau']),\n",
    "          'eta':test_point['eta']}\n",
    "\n",
    "bx_jax = bx.Model(\n",
    "    log_density=elbo,\n",
    "    test_point=test_point,\n",
    "    transform_fn=transform_fn)\n",
    "\n",
    "idata_jax=bx_jax.mcmc.numpyro_nuts(seed=jax.random.key(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
